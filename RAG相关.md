RAG

用户-》LLM检索与理解-》检索系统

## 【RAG的基础流程】

```
原始文档 → 分片（Chunking）→ 向量化（Embedding）→ 建立索引 → 查询 → 召回相关片段 → LLM 生成答案
```

**RAG 的检索效果 ≈ 向量质量  × 检索设计 × 上下文处理质量**

### 

## 【查询】

### 查询重写

将用户原始query**转换为更标准、更有助于检索或理解的版本**，通常在语义不清、表达标准或缺乏上下文时使用

作用：

- 统一表达 （口语 ->书面）

- 增强语义对齐

- 修复用户拼写、结构问题

示例：

- 原始：`李白是哪的人啊？`

- 重写：`李白的出生地是哪里？`

**技术方式：**

- 使用语言模型（如 T5、ChatGPT）

- 提前定义规则或模板

- 使用查询历史 + 检索上下文增强重写



### 查询分解

将一个复杂、多目标的query拆分为多个子查询，**逐个处理再合成结果**，适合“多跳推理”或“长问题”

**示例：**  
用户 query：

> “曹操的儿子是谁，他打了什么仗？”

分解为：

1. 曹操的儿子是谁？

2. 曹操的儿子打过什么仗？

**技术方式：**

- 使用 Prompt 分解 (`decompose this query into independent sub-questions`)

- 使用结构分析器（如依存句法分析）



### 查询澄清

当query歧义、不完整或语义不清晰时，系统主动提出补充问题，帮助用户明确意图。

**作用：**

- 避免误检（例如"Python"是语言还是动物）

- 提高交互式系统准确性

- 改进冷启动体验

**示例：**  
用户 query：

> “苹果是什么？”

系统澄清：

> 你指的是“水果”还是“Apple 公司”？

**技术方式：**

- 使用意图分类器 + 歧义词库

- LLM 判断语义分歧点

- 与用户交互澄清（RAG+Chat）

**适用场景：**

- 智能客服、语音助手

- 教育、金融等需要精准交互的场景



### 查询扩展

在用户query的基础上，增加**同义词、上下位词、相关词**以扩大召回范围

**作用：**

- 弥补用户关键词不足

- 提高召回率

- 应对词语变体问题

**示例：**  
用户 query：

> “退烧药”

扩展为：

> “退热药”、“解热药”、“布洛芬”、“对乙酰氨基酚”

**技术方式：**

- 使用嵌入距离扩展（向量近邻）

- 基于知识图谱的实体扩展

- 利用预训练模型（如 BERT、Word2Vec）找近义词

**适用场景：**

- 搜索引擎

- 医疗、法律、专业语境的 query 丰富化

- 查询意图增强



## 【分块技术】

#### 一、结构的分块技术（规则分块）

##### 固定长度分块

每 N 个字符或 tokens 切分一次。

##### 句子级分块

利用 NLP 工具（如 spaCy、nltk）将文本按句子划分，合并若干句子为一个 chunk。

##### 段落级分块

根据换行、空行、缩进符号来切分。

##### 层级递归分块（RecursiveCharacterTextSpliter）

- **方法：** 按优先级尝试用不同的分隔符递归切分。

- **分隔符：** ["\n\n", "\n", ".", "。", " ", ""]



### 二、按语义的分块技术（智能分块）

#### 语义聚类分块

- **方法：** 将文本按句子切分后，使用嵌入模型对句子向量化，再根据向量相似度进行聚类或拼接。

- **优点：** 高保真度的语义连续块。

- **缺点：** 耗资源、需要嵌入模型。

- **适用场景：** 高质量文档索引、学术/技术类问答。



#### 基于提示的分块

- **方法：** 结合上下文，判断 chunk 是否“对回答问题有价值”；只保留关键部分。

- **工具：** 可用 LLM 判断 chunk 的语义密度。

- **适用场景：** 构建 RAG 优先上下文窗口、精准回答。



#### LLM引导分块

- **方法：** 用大模型直接判断分割点：
  
  Prompt: "请将以下文本按语义完整性分成若干段落：... "

- **优点：** 可处理非结构化混乱文本。

- **缺点：** 慢、成本高。

- **适用场景：** 高价值内容处理，如法律、医疗文本。



### 三、分块控制策略

- **Chunk Size：** 每个 chunk 的最大长度（建议 256～1024 tokens）

- **Chunk Overlap：** 每个 chunk 与下一个之间重叠的内容（常用 10%～30%）

- **滑动窗口（sliding window）：** 用于连续语义保持（尤其适合新闻、小说）





## 【分片（chunking）】

#### 1、整体切片

避免语义分割的分片方式

按传统的500token进行分片可能出现问题：

- 一句话被切断

- 一个段落被打碎

- 多个相关句子分散到不同片段，影响召回准确性

整体分片：

- 把语义上有内在关联的一段内容，作为一个整体去切片

- 如，不安固定数字，而是按段落、标题、Q&A对等自然结构切分

- 使用MarkdownHeaderTextSplitter或SemanticChunker

#### 2、建立相关联的索引

让相关chunk可以相互联系或一起被召回

有时候一个问题涉及多个片段（例如问题分布在第一和第4节），所以我们要;

- 把相关片段组织在一起(比如带上上下文)

- 或者设计索引方式，使得它们能“组团”被检索出来

方法:

| 方法                | 描述                         |
| ----------------- | -------------------------- |
| parent_document索引 | 将多个子chunk建立指向“父文档”的关联      |
| 多chunk查询扩展        | 向量召回多个chunk,拼接上下文          |
| graph-based索引     | 建图结构，如章节之间的引用关系            |
| metadata辅助索引      | 给chunk加metadata(如章节号、主题标签) |

#### 3、Document对象

分片产生的是一个个Document对象，本质上也是属于pydantic的模型（BaseModel）

使用model_dump可见其基本字段：

```json
{
    "id": null,
    "metadata": {},
    "page_content": "对象的内容",
    "type": "Document"
}
```



## 【向量化（Embedding）】

将文本转成向量，然后存储到向量数据库

#### 向量检索方式

- 将用户查询语句转为向量

- 与向量库中的所有向量作相似度比较

- 选出top-k最相关向量

- 作为上下文输入到大模型（RAG生成阶段）



### 向量化使用的关键位置

- 文档预处理： 将chunks向量化入库

- 查询阶段： 把用户query向量化检索

- 多轮会话跟踪： 对历史query嵌入建索引

- rerank阶段： 使用向量相似度 + rerank模型筛选结果



### 嵌入模型

将文本转为向量的模型



### 密集嵌入与稀疏嵌入

#### 密集嵌入：

向量维度少（如384、768），几乎每个维度都有值，靠**语义相似性**找相似文本

举例模型：bge-m3, text-embedding-ada-002, all-MiniLM-L6-v2, qwen-embedding

特点：每个文本被编码成一个长度固定的、稠密的向量，支持模糊语义查询

优点：强语义理解能力， 能识别同义体、语义变体

#### 稀疏嵌入

向量维度很大（成千上万），大部分维度为0，靠**关键词匹配**找相似文本

举例模型：TF-IDF/BM25(传统信息检索方法)

SPLADE(现代稀疏向量模型)

CoIBERT(低维稀疏)

特点：每个文本被转化为一个非常长的向量，只有少数维度非零

优点： 对关键词、实体、专有名称召回强



#### 实际应用：

混合检索： 最终召回得分 = α * 稠密相似度 + β * 稀疏相似度



多阶段检索： 

- 稀疏检索先过滤候选文档（快速、精准）

- 再用稠密检索排序（语义更强）

- 



## 【建立索引】

### 1、父子文档索引

父文档索引  = 在分片时，保留每个小片段（子文档）和原始大段（父文档）之间的关联

最终只对子文档向量化，但检索时返回完整的父文档作为上下文



### 2、分层索引

分层索引是将文档按“多级粒度”组织和索引的一种方式

它结合粗粒度（章节/段落）与细粒度（句子/短语）索引，使得RAG检索更智能、更灵活

第一层： 快速缩小搜索范围

第二层：在缩小的范围内作精确匹配



### 3、多表示索引

一种适用于复杂查询、多意图、多表达场景的索引优化策略

多表示索引是指对同一文本片段，生成多个“表达方式”进行向量化入库，从而提高模型对不同查询方式的理解和召回能力。



多表示索引 =  同一语义片段 + 多种表达方式的向量嵌入

例子;

比如原文是：

> “员工可以通过企业VPN访问内网资源。”

用户可能问：

- “如何远程连接公司网络？”

- “访问内网有哪些方法？”

- “VPN连接公司资源的方式有哪些？”







## 【向量检索】

**向量检索（Vector Search）** 是一种基于**语义相似度**进行信息检索的方法。它不依赖关键词匹配，而是将文本、图像、代码等数据转换成向量（embedding），通过向量之间的**距离**来判断相似度，从而实现检索。

- 用户输入 query，系统将其也嵌入成向量，计算其与所有文档向量的相似度（如余弦相似度）。

- 返回相似度 Top-K 的文档作为候选结果。





## 【BM25检索】

传统信息检索中最常用、最经典的稀疏检索方法之一

BM25算法是一种基于词频统计的稀疏向量检索算法，用于衡量查询和文档之间的匹配度。

### 核心思想

 当用户查询一个短语时，BM25会根据每个文档中包含该查询词的频率，以及这个词在整个语料中出现的频率，计算出一个相关性得分。





## 【Rerank】

重排序，指的是在检索系统中，初步检索返回一批候选结果之后，使用更强大的模型对这些候选结果进行重排序，以提升最终结果的质量。**用于提高从向量数据库检索出来的文档与用户查询的匹配度**。

### Rerank常见的方式

| 方法                      | 简述                                                      | 特点                                   |
| ----------------------- | ------------------------------------------------------- | ------------------------------------ |
| **Cross-Encoder**       | 把 query 和每一个候选文档拼接在一起输入到一个 Transformer 模型中，让模型判断相关性得分   | 精度高、速度慢，适合 rerank Top-K（通常 K=20~100） |
| **Bi-Encoder + Sort**   | 初始阶段使用双塔（Bi-Encoder）嵌入模型，再在候选集内做一次排序                    | 较快，但精度不如 Cross-Encoder               |
| **Hybrid（稀疏+稠密）Rerank** | 将稀疏检索（BM25）和向量检索结果合并，用重排序模型融合评分                         | 兼顾召回率与相关性                            |
| **LLM-based Scoring**   | 使用大型语言模型（如 ChatGPT、Qwen、Q-former 等）对文档与 query 相关性进行评分排序 | 成本高但效果好，适用于关键任务                      |



### 常见Rerank模型

| 模型名                           | 特点                                  | 来源        |
| ----------------------------- | ----------------------------------- | --------- |
| **bge-reranker-base / large** | 中文和英文都支持，适用于向量检索后的 rerank           | BAAI      |
| **colbert**                   | 利用词粒度匹配，高效准确                        | Stanford  |
| **Cohere Reranker**           | 商用效果很好，API 支持多语言                    | Cohere.ai |
| **ms-marco-MiniLM-L6**        | Hugging Face 上经典英文 reranker         | Microsoft |
| **GPT/LLM-based reranker**    | 利用 GPT 类模型评分，例如 prompt: "是否相关？1~5分" | 自定义       |







## 【响应生成】

#### 关键技术

##### 1、上下文拼接（context construction）

把多个检索到的chunk合并为提示输入prompt:

```textile
You are a helpful assistant.

Context:
1. <chunk1 content>
2. <chunk2 content>
...

Question: {用户问题}

Answer:

```

**关键点：**

- 排序：按相关度、时间线、可信度排序

- 剪裁：只保留最关键的 chunk（控制 token 长度）

- 过滤：可加入知识质量过滤器



##### 2、Prompt设计（Prompt Enginnering）

Prompt直接影响模型响应的效果，常见的设计策略：

- **Zero-shot**：只给 context 和 question，不加范例

- **Few-shot**：加上几个问答对作为示例

- **Instruction-style**：加上任务明确提示，如：
  
  > “基于以下上下文，准确、简洁地回答用户问题。”

- **格式模板提示（template prompting）**：

```textile
Context:
{context}

Task:
请根据上述资料回答下列问题，不要编造，如果无法回答请说“无法确定”。

Q: {question}
A:

```



##### 3、Answer Chain 构建（链式推理）

有时问题需要多步推理才能回答：

- **Chain-of-Thought（CoT）提示**

- **ReAct Prompting**（结合 reasoning + action）

- **逐步解题模板**（Step-by-step）

示例CoT:

```textile
Based on the following context, think step-by-step to answer the question:

Context:
{context}

Question:
{question}

Answer:

```



## 【检索评估】

衡量返回的文本片段是否足够相关、完整、准确，能为下游生成提供有用的信息。

### 评估的目的

- 衡量检索器是否找到了**对用户问题有帮助的上下文**

- 评估向量数据库是否构建得合理（chunk 粒度/embedding 质量）

- 优化 chunking/embedding/retriever 的组合

- 与不同 embedding 模型和分割策略做对比测试



### RAG检索评估的主要方式

#### 1、离线评估

针对已有的数据集进行系统测试，常见指标如下：

| 指标                            | 含义                       | 用途                      |
| ----------------------------- | ------------------------ | ----------------------- |
| **Recall@K**(召回率)             | Top-K 中是否包含 Ground Truth | 最常用，衡量命中率               |
| **Precision@K**（精确率）          | Top-K 中正确内容的比例           | 在某些 QA 场景有用             |
| **Mean Reciprocal Rank（MRR）** | 正确文档排名的倒数的平均值            | 排序评估                    |
| **NDCG@K（归一化折损累积增益）**         | 排名相关性的评估                 | 适合 graded relevance 的情况 |
| **Hit@K**                     | 是否在 Top-K 命中             | 与 Recall 类似，但简单         |

##### 召回率

定义：

> **召回率（Recall）** 是衡量一个系统从所有相关结果中成功找回了多少的指标。

数学公式：

Recall=True Positives/(False Negatives+True Positives)​

- **True Positives (TP)**：被系统正确检索到的相关文档数量

- **False Negatives (FN)**：未被系统检索到的但实际上相关的文档数量

 示例：

假设某个问题 Q 有 3 个 ground truth 支撑文档（D1、D2、D3），系统实际返回了 D1、D4、D5：

- TP = 1（D1）

- FN = 2（D2、D3）

那么：

Recall=1/(2+1)​=0.33

##### 精确率

**精确率（Precision）** 是衡量系统返回的结果中有多少是**真正相关的**。

假设用户问一个问题，系统返回了 5 个文档：

- 其中有 3 个是对问题有帮助的（TP）

- 另外 2 个是无关的（FP）

那么：

Precision=3/(2+3)​=0.6

表示这些结果中，**60% 是有用的**。



##### MRR (平均倒数排名)

**MRR（Mean Reciprocal Rank）** 是信息检索和问答系统中常用的评价指标，用来衡量第一个相关结果在结果列表中排名的位置。

它关注的是**第一个相关结果**出现得有多早（越靠前越好）



$MRR=1/Q * ∑(1/ranki)$

- ∣Q∣：查询（query）的总数

- ranki​：对于第 i 个查询，第一个相关文档在返回列表中的排名（从 1 开始）



##### MAP(平均精度)

Mean Average Precision

**MAP** 是衡量一组查询中检索结果的整体排序质量的指标，尤其在多相关项的场景中非常有用。

相比于 MRR 只关注第一个相关文档，**MAP 会考虑多个相关文档的位置**，它是对每个查询的 **AP（Average Precision，平均精度）** 求平均。





#### 2、人工评估

人类标注者判断返回内容的：

- **相关性**

- **上下文完整性**

- **歧义性**

- **冗余与干扰**

常用标注等级：

- 0 分：完全无关

- 1 分：有些相关

- 2 分：高度相关，支撑回答

> 人工评估可以作为高质量基准，用于训练打分器或自动打标。



#### 3、问答生成质量评估（Indirect）

评估“检索 + 生成”整体效果，从侧面衡量检索质量：

- 用 BLEU、ROUGE、EM、F1 来评估最终答案和 ground truth 的一致性

- 通过 ablation 实验，比较不同 retriever 带来的回答差异











## 【其他】

### 1、Text2SQL

是指将自然语言问题转换为机器可执行的SQL查询语句，如“列出 2023 年销售额最高的产品”，转换为“SELECT product_name FROM sales WHERE year=2023 ORDER BY revenue DESC LIMIT 1;”

### 2、图数据库+知识图谱+RAG

将图数据库与知识图谱集成到整个AI系统或RAG系统中，以增强对实体、关系和推理的理解与查询能力。

### 3、有权限的RAG系统

有权限的RAG = 检索增强问答 + 用户身份验证 + 数据访问授权机制

```textile
┌────────────┐        ┌───────────────┐        ┌──────────────┐
│  用户请求   ├───────▶  权限校验模块      ├───────▶  向量检索模块 │
└────────────┘        └────┬────────────┘      └────┬──────────┘
                           │                             │
                           ▼                             ▼
                  权限过滤后的索引库           权限过滤后的文档上下文
                           │                             │
                           └────▶ LLM 生成回答（附上下文）◀─────┘
```

#### 设计核心

- 用户登录+角色

每个用户或角色绑定一个或多个【权限标签】或【资源作用域】

```json
{
  "user": "zhangsan",
  "role": "finance",
  "permissions": ["dept:finance"]
}
```

- 向量数据库设计： 加入权限到metadata中

文档分片之后，在每个chunk存入向量数据库是，在metadata中加入权限字段

```json
doc = Document(
  page_content="公司2024年财务预算...",
  metadata={"department": "finance", "owner": "zhangsan"}
)
```

- 检索时按用户权限过滤向量结果

系统根据用户身份加载其权限标签

然后向量搜索时加入filter

```python
retriever = vectorstore.as_retriever(
    search_kwargs={"k": 5, "filter": {"department": "finance"}}
)
```

- 模板填充的时候只显示权限范围内的上下文

- 高级拓展： 按文档分组隔离向量库。
