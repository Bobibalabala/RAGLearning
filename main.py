"""
@Author: bo.yang-a2302@aqara.com
@Date: 

"""
from langchain_community.vectorstores import FAISS
from langchain_core.messages import BaseMessage
from langchain_openai import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document

import os
import json

from openai import api_key
from pydantic import SecretStr

# 1. è®¾ç½®æ¨¡å‹å‚æ•°
EMBED_MODEL = "text-embedding-ada-002"
# EMBED_MODEL = "bge-m3:latest"
EMBED_BASE_URL = "http://43.173.190.109:1111/v1"
EMBED_API_KEY = "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"

LLM_MODEL = "Qwen/Qwen3-30B-A3B"
# LLM_MODEL = "qwen3:4b"
LLM_BASE_URL = "http://10.11.40.253:8002/v1"
LLM_API_KEY = "<KEY>"

VECTOR_DB_PATH = "./faiss_index"

# 2. æ–‡æ¡£åŠ è½½ä¸åˆ†ç‰‡ï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰
def load_and_index_docs(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        text = f.read()

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=100,
        chunk_overlap=10,
    )

    docs = [Document(page_content=chunk) for chunk in splitter.split_text(text)]
    # å‘é‡åŒ–
    embedding = OpenAIEmbeddings(
        model=EMBED_MODEL,
        api_key=SecretStr(EMBED_API_KEY),
        base_url=EMBED_BASE_URL
    )
    vectorstore = FAISS.from_documents(docs, embedding)

    # ä¿å­˜å‘é‡æ•°æ®åº“
    vectorstore.save_local(VECTOR_DB_PATH)
    print("âœ… å‘é‡æ•°æ®åº“å·²ä¿å­˜")

# 3. åŠ è½½å‘é‡åº“ & æ£€ç´¢
def retrieve_docs(querys, top_k=3):
    embedding = OpenAIEmbeddings(
        model=EMBED_MODEL,
        api_key=SecretStr(EMBED_API_KEY),
        base_url=EMBED_BASE_URL
    )
    vectorstore = FAISS.load_local(VECTOR_DB_PATH, embedding, allow_dangerous_deserialization=True)
    final_results = []
    ids = []
    for query in querys:
        results = vectorstore.similarity_search(query, k=top_k)
        for result in results:
            if result.id not in ids:
                final_results.append(result)
                ids.append(result.id)
    return final_results

def generate_mult_query(query):
    prompt_llm_query = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çš„æŸ¥è¯¢æ‰©å±•åŠ©æ‰‹ï¼Œä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·çš„åŸå§‹é—®é¢˜ï¼Œç”Ÿæˆå¤šä¸ªè¯­ä¹‰ç›¸è¿‘ä½†è¡¨è¾¾ä¸åŒçš„é—®å¥ï¼Œä»¥ä¾¿æ›´å¥½åœ°è¦†ç›–æœç´¢æ„å›¾å’Œè¯­ä¹‰åŒ¹é…ã€‚

    ç”¨æˆ·é—®é¢˜ï¼š"{query}"

    è¯·åŸºäºè¿™ä¸ªé—®é¢˜ç”Ÿæˆ 5 ä¸ªä¸åŒçš„ã€è¯­ä¹‰ç›¸è¿‘çš„è‡ªç„¶è¯­è¨€é—®é¢˜ï¼Œè¦æ±‚ï¼š
    1. ä¿æŒåŸæ„ä¸å˜ï¼›
    2. å¥å¼å¤šæ ·ï¼›
    3. åŒ…å«ç”¨æˆ·å¯èƒ½ä½¿ç”¨çš„ä¸åŒæé—®æ–¹å¼æˆ–å…³é”®è¯ã€‚

    è¯·ä»¥ JSON æ•°ç»„æ ¼å¼è¿”å›ï¼Œä¾‹å¦‚ï¼š
    [
      "ç›¸ä¼¼é—®é¢˜1",
      "ç›¸ä¼¼é—®é¢˜2",
      ...
    ]
    """
    # print("*" * 80)
    # print(prompt_llm_query)
    # print("*" * 80)

    llm = ChatOpenAI(
        model=LLM_MODEL,
        base_url=LLM_BASE_URL,
        api_key=SecretStr(LLM_API_KEY),
        temperature=0.7,
        top_p=0.8,
        max_tokens=1000,
        presence_penalty=2,
        # extra_body={"chat_template_kwargs": {"enable_thinking": False}}
    )
    result: BaseMessage = llm.invoke(prompt_llm_query)
    query = result.content
    query = json.loads(query)
    return query



# 4. æ„é€  Prompt å¹¶ç”¨ qwen3 å›ç­”
def generate_answer(query, docs):
    context = "\n\n".join([doc.page_content for doc in docs])
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä»¥ä¸‹å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼š
========
{context}
========
é—®é¢˜ï¼š{query}
è¯·ç”¨ç®€æ´å’Œå‡†ç¡®çš„è¯­è¨€å›ç­”ï¼š"""
    llm = ChatOpenAI(
        model=LLM_MODEL,
        base_url=LLM_BASE_URL,
        api_key=SecretStr(LLM_API_KEY),
        temperature=0.0,
        top_p=0.8,
        max_tokens=32768,
        presence_penalty=1.5,
        extra_body={"chat_template_kwargs": {"enable_thinking": False}}
    )
    return llm.invoke(prompt).content

# ======== ä¸»æµç¨‹ï¼ˆç¬¬ä¸€æ¬¡è¿è¡Œæ—¶æ‰§è¡Œåˆ†ç‰‡ï¼‰========
if __name__ == "__main__":
    file_path = "C:/Users/admin/Desktop/test2.txt"  # æ›¿æ¢ä¸ºä½ çš„æ–‡æ¡£è·¯å¾„
    if not os.path.exists(VECTOR_DB_PATH):
        load_and_index_docs(file_path)

    user_query = "è¿™äº§å“éƒ½é€‚åˆå“ªäº›äºº"
    user_querys = generate_mult_query(user_query)
    relevant_docs = retrieve_docs(user_querys)
    answer = generate_answer(user_query, relevant_docs)

    print("\nğŸ’¡ å›ç­”ï¼š", answer)